{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -qU \"langchain[groq]\" sentence-transformers duckduckgo-search langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain.tools import Tool\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session memory to store previous messages and state\n",
    "session_memory = defaultdict(lambda: {\"messages\": [], \"state\": None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an AI assistant designed to handle various tasks including:\n",
    "- Answering general knowledge questions.\n",
    "- Performing mathematical calculations.\n",
    "- Retrieving real-time information from the web.\n",
    "- Providing research-based answers from the vector database.\n",
    "- Handling natural language queries efficiently.\n",
    "- Generating or optimizing SQL queries.\n",
    "\n",
    "Instructions:\n",
    "- Always keep the response concise and clear.\n",
    "- If the user asks for real-time information, prioritize the web search tool.\n",
    "- If the query relates to mathematical expressions, use the calculator tool.\n",
    "- If the user seeks in-depth or document-based information, use the RAG tool.\n",
    "- If the user requests to generate or optimize an SQL query, **only return the SQL query itself without any extra text, explanations, or assumptions**.\n",
    "- For everything else, generate a response using the LLM.\n",
    "\n",
    "Be accurate, helpful, and concise.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the API key is already set\n",
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ChatGroq LLM with the system prompt\n",
    "llm = ChatGroq(\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_response(input_text, session_id):\n",
    "    # Retrieve or initialize session memory\n",
    "    if session_id not in session_memory:\n",
    "        session_memory[session_id] = {\"messages\": []}\n",
    "\n",
    "    messages = session_memory[session_id][\"messages\"]\n",
    "\n",
    "    # Add user message\n",
    "    messages.append({\"role\": \"user\", \"content\": input_text})\n",
    "\n",
    "    # Invoke the LLM with context\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    # Append assistant response to maintain context\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
    "\n",
    "    # Return the response text\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define table schemas\n",
    "table_schemas = \"\"\"\n",
    "Users(id, name, email, created_at)\n",
    "Products(id, name, price, created_at)\n",
    "Orders(id, order_date, total_amount, user_id)\n",
    "\"\"\"\n",
    "\n",
    "# Function to convert natural language to SQL query\n",
    "def nl_to_sql(text: str, session_id: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    Convert the following natural language command into an SQL query.\n",
    "    Use the following table schemas:\n",
    "\n",
    "    {table_schemas}\n",
    "\n",
    "    Command: {text}\n",
    "\n",
    "    Provide only the SQL query without any explanation or additional text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate SQL query using optimize_response\n",
    "    response = optimize_response(prompt, session_id)\n",
    "    return response.strip()\n",
    "\n",
    "# Create the tool\n",
    "nl_to_sql_tool = Tool(\n",
    "    name=\"Natural Language to SQL\",\n",
    "    func=lambda text, session_id: nl_to_sql(text, session_id),\n",
    "    description=\"\"\"\n",
    "    This tool strictly converts natural language commands into raw SQL queries \n",
    "    based on the provided table schemas without adding any explanation, assumptions, \n",
    "    or additional context. The output must be a valid SQL query only.\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "def search(text: str, session_id: str) -> str:\n",
    "    result = search_tool.invoke(text)\n",
    "    return optimize_response(result, session_id)\n",
    "\n",
    "web_search_tool = Tool(\n",
    "    name=\"DuckDuckGo Search\",\n",
    "    func=lambda text: search(text, str(uuid.uuid4())),\n",
    "    description=\"Search DuckDuckGo for real-time information.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculator(inputs: str, session_id: str):\n",
    "    try:\n",
    "        result = str(eval(inputs))\n",
    "        return optimize_response(result, session_id)\n",
    "    except Exception as e:\n",
    "        return optimize_response(f\"Error: {e}\", session_id)\n",
    "\n",
    "calculator_tool = Tool(\n",
    "    name=\"Calculator\",\n",
    "    func=lambda text: calculator(text, str(uuid.uuid4())),\n",
    "    description=\"Performs basic arithmetic calculations. Input should be a mathematical expression.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -qU langchain_community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "def load_document(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "documents = load_document('./data/temp.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -qU langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "\n",
    "connection = \"postgresql+psycopg://postgres:postgres@localhost:5431/vectordb\"\n",
    "collection_name = \"my_docs\"\n",
    "\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_into_vectordb(splitted_texts):\n",
    "    documents = []\n",
    "    ids = []\n",
    "    for i, text in enumerate(texts):\n",
    "        ids.append(i)\n",
    "        documents.append(Document(page_content=text.page_content))\n",
    "    vector_store.add_documents(documents=documents, ids=ids)\n",
    "\n",
    "store_into_vectordb(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_tool_func(text: str, session_id: str) -> str:\n",
    "    results = vector_store.similarity_search(query=text,k=1)\n",
    "    docs = \"\"\n",
    "    for doc in results:\n",
    "        docs+=f\"{doc.page_content} [{doc.metadata}]\"\n",
    "    return optimize_response(docs, session_id)\n",
    "\n",
    "rag_tool = Tool(\n",
    "     name=\"Research on Machine Learning Algorithms and Feature Extraction for Time Series\",\n",
    "     func=lambda text: rag_tool_func(text, str(uuid.uuid4())),\n",
    "     description=\"Research on Machine Learning Algorithms and Feature Extraction for Time Series\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    output: str\n",
    "    session_id: str\n",
    "\n",
    "graph = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_query(state):\n",
    "    query = state[\"input\"].lower()\n",
    "    if any(x in query for x in [\"calculate\", \"+\", \"-\", \"*\", \"/\"]):\n",
    "        return \"calculator\"\n",
    "    elif any(x in query for x in [\"Time Series\", \"Machine Learning\", \"Feature Extraction\"]):\n",
    "        return \"rag\"\n",
    "    elif any(x in query for x in [\"search\", \"web\", \"information\"]):\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        return \"groq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x180c18512b0>"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modified node functions to pass session_id\n",
    "graph.add_node(\"calculator\", lambda state: {\n",
    "    \"output\": calculator_tool.run(state[\"input\"]),\n",
    "    \"session_id\": state[\"session_id\"]\n",
    "})\n",
    "\n",
    "graph.add_node(\"web_search\", lambda state: {\n",
    "    \"output\": web_search_tool.run(state[\"input\"]),\n",
    "    \"session_id\": state[\"session_id\"]\n",
    "})\n",
    "\n",
    "graph.add_node(\"rag\", lambda state: {\n",
    "    \"output\": rag_tool.run(state[\"input\"]),\n",
    "    \"session_id\": state[\"session_id\"]\n",
    "})\n",
    "\n",
    "graph.add_node(\"groq\", lambda state: {\n",
    "    \"output\": optimize_response(state[\"input\"], state[\"session_id\"]),\n",
    "    \"session_id\": state[\"session_id\"]\n",
    "})\n",
    "\n",
    "graph.add_node(\"nl_to_sql\", lambda state: {\n",
    "    \"output\": nl_to_sql_tool.run(state[\"input\"], state[\"session_id\"]),\n",
    "    \"session_id\": state[\"session_id\"]\n",
    "})\n",
    "\n",
    "# Enhanced function to strictly get SQL without text\n",
    "def force_sql_output(state):\n",
    "    \"\"\"\n",
    "    - If `nl_to_sql` gives valid SQL -> terminate.\n",
    "    - If `nl_to_sql` fails -> directly ask Groq to generate SQL without text.\n",
    "    \"\"\"\n",
    "    if state[\"output\"]:\n",
    "        return END\n",
    "    return \"groq\"\n",
    "\n",
    "# Route the queries based on input type\n",
    "graph.add_conditional_edges(\n",
    "    START,\n",
    "    route_query,\n",
    "    {\n",
    "        \"calculator\": \"calculator\",\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"rag\": \"rag\",\n",
    "        \"groq\": \"groq\",\n",
    "        \"nl_to_sql\": \"nl_to_sql\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Handle SQL generation failure gracefully by falling back to Groq\n",
    "graph.add_conditional_edges(\n",
    "    \"nl_to_sql\",\n",
    "    force_sql_output,\n",
    "    {\n",
    "        END: END,\n",
    "        \"groq\": \"groq\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Final edges to terminate the flow\n",
    "graph.add_edge(\"calculator\", END)\n",
    "graph.add_edge(\"web_search\", END)\n",
    "graph.add_edge(\"rag\", END)\n",
    "graph.add_edge(\"groq\", END)\n",
    "graph.add_edge(\"nl_to_sql\", END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the graph\n",
    "final_graph = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: name 'Image' is not defined\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display  \n",
    "\n",
    "try:  \n",
    "    graph_image = final_graph.get_graph().draw_mermaid_png()  \n",
    "    display(Image(graph_image))  \n",
    "except Exception as e:  \n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a session_id for the conversation\n",
    "session_id = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: A time series is a sequence of\n",
      "data points, measured typically at successive times, spaced at uniform time intervals. It can be thought of as a list of observations recorded over time. Time series data can come from various sources, such as economic indicators, weather patterns, stock prices, or sensor readings.\n",
      "\n",
      "Time series analysis involves the study of time series data to extract meaningful patterns, trends, and relationships. This analysis can help in forecasting future values of the time series, understanding the underlying factors driving the data, and making informed decisions based on the insights gained.\n",
      "\n",
      "Time series data can be univariate, meaning it consists of a single variable measured over time, or multivariate, meaning it consists of multiple variables measured over time. Time series analysis techniques can be divided into two categories: time-domain methods and frequency-domain methods. Time-domain methods focus on the time-based relationships between data points, while frequency-domain methods focus on the frequency-based relationships between data points.\n",
      "\n",
      "Some common time series analysis techniques include moving averages, exponential smoothing, autoregressive integrated moving average (ARIMA), and state-space models. These techniques can be used to model and forecast time series data, as well as to identify and analyze trends, seasonality, and other patterns in the data.\n"
     ]
    }
   ],
   "source": [
    "# First question\n",
    "response = final_graph.invoke({\n",
    "    \"input\": \"What is Time Series?\",\n",
    "    \"session_id\": session_id\n",
    "})\n",
    "print(\"Output:\", response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: Time series analysis has a wide range of use cases across various industries and applications. Here are some examples:\n",
      "\n",
      "1. Economic Forecasting: Time series analysis is widely used in economics to forecast economic indicators such as GDP, inflation, unemployment rate, and stock prices. Economists use time series models to understand the relationships between economic variables and to make predictions about future economic conditions.\n",
      "2. Weather Forecasting: Time series analysis is used in meteorology to forecast weather patterns such as temperature, precipitation, and wind speed. Meteorologists use time series models to analyze historical weather data and make predictions about future weather conditions.\n",
      "3. Sales Forecasting: Time series analysis is used in business to forecast sales revenue, product demand, and customer behavior. Business analysts use time series models to analyze historical sales data and make predictions about future sales trends.\n",
      "4. Quality Control: Time series analysis is used in manufacturing to monitor and control the quality of products. Manufacturers use time series models to analyze production data and identify trends or anomalies that may indicate a problem with the production process.\n",
      "5. Financial Risk Management: Time series analysis is used in finance to analyze financial data such as stock prices, interest rates, and exchange rates. Financial analysts use time series models to identify risks and opportunities in financial markets and to make informed investment decisions.\n",
      "6. Healthcare Monitoring: Time series analysis is used in healthcare to monitor patient health and to detect anomalies in physiological signals. Healthcare professionals use time series models to analyze patient data and identify trends or patterns that may indicate a health problem.\n",
      "7. Social Media Analysis: Time series analysis is used in social media analysis to analyze user behavior and to identify trends in social media data. Social media analysts use time series models to analyze user data and make predictions about future user behavior.\n",
      "\n",
      "These are just a few examples of the many use cases of time series analysis. Time series analysis is a powerful tool that can be used to analyze and forecast data in many different contexts.\n"
     ]
    }
   ],
   "source": [
    "# Follow-up question\n",
    "response = final_graph.invoke({\n",
    "    \"input\": \"Can you explain its use cases?\",\n",
    "    \"session_id\": session_id\n",
    "})\n",
    "print(\"Output:\", response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: Assuming you have a database schema with two tables, `users` and `orders`, where the `orders` table has a foreign key `user_id` that references the `id` column in the `users` table, and the `orders` table has a `created_at` column that stores the date and time when each order was created, the following SQL query will retrieve the users who have placed orders in the last 30 days:\n",
      "```vbnet\n",
      "SELECT u.*\n",
      "FROM users u\n",
      "JOIN orders o ON u.id = o.user_id\n",
      "WHERE o.created_at >= NOW() - INTERVAL 30 DAY;\n",
      "```\n",
      "This query joins the `users` and `orders` tables on the `user_id` and `id` columns, respectively, and filters the results to only include orders that were created in the last 30 days using the `WHERE` clause. The `NOW()` function returns the current date and time, and the `INTERVAL 30 DAY` expression subtracts 30 days from the current date and time. The resulting set of rows includes the user data for all users who have placed orders in the last 30 days.\n"
     ]
    }
   ],
   "source": [
    "response = final_graph.invoke({\n",
    "    \"input\": \"Generate a sql query for users who have placed orders in the last 30 days\",\n",
    "    \"session_id\": session_id\n",
    "})\n",
    "print(\"Output:\", response[\"output\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
