{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h3>Git Clone</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Cloning repo from https://github.com/sivaprasadreddy/spring-modular-monolith.git to ./monolith_code...\n",
      "[SUCCESS] Repo cloned successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def clone_repo(repo_url: str, destination_dir: str = \"./cloned_repo\"):\n",
    "    try:\n",
    "        if os.path.exists(destination_dir):\n",
    "            print(f\"[INFO] Destination '{destination_dir}' already exists. Removing it first...\")\n",
    "            subprocess.run([\"rm\", \"-rf\", destination_dir], check=True)\n",
    "\n",
    "        print(f\"[INFO] Cloning repo from {repo_url} to {destination_dir}...\")\n",
    "        subprocess.run([\"git\", \"clone\", repo_url, destination_dir], check=True)\n",
    "        print(\"[SUCCESS] Repo cloned successfully.\")\n",
    "        return destination_dir\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"[ERROR] Failed to clone repository: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    repo_url = \"https://github.com/sivaprasadreddy/spring-modular-monolith.git\"  # replace this\n",
    "    destination = \"./monolith_code\"  # change if you want a different dir\n",
    "    clone_repo(repo_url, destination)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h3>File Analysis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\BookStoreApplication.java[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\catalog\\ProductApi.java\n",
      "\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\catalog\\ProductDto.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\catalog\\domain\\ProductEntity.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\catalog\\domain\\ProductNotFoundException.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\catalog\\domain\\ProductRepository.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\catalog\\domain\\ProductService.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\catalog\\mappers\\ProductMapper.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\catalog\\web\\CatalogExceptionHandler.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\catalog\\web\\ProductRestController.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\catalog\\web\\ProductWebController.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\common\\models\\PagedResult.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\config\\RabbitMQConfig.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\inventory\\InventoryEntity.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\inventory\\InventoryRepository.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\inventory\\InventoryService.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\inventory\\OrderEventInventoryHandler.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\notifications\\OrderEventNotificationHandler.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\orders\\CreateOrderRequest.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\orders\\CreateOrderResponse.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\orders\\InvalidOrderException.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\orders\\OrderDto.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\orders\\OrderNotFoundException.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\orders\\OrdersApi.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\orders\\OrderView.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\orders\\domain\\OrderEntity.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\orders\\domain\\OrderRepository.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\orders\\domain\\OrderService.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\orders\\domain\\models\\Customer.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\orders\\domain\\models\\OrderCreatedEvent.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\orders\\domain\\models\\OrderItem.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\orders\\domain\\models\\OrderStatus.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\orders\\mappers\\OrderMapper.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\orders\\web\\Cart.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\orders\\web\\CartController.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\orders\\web\\CartUtil.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\orders\\web\\OrderForm.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\orders\\web\\OrderRestController.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\orders\\web\\OrdersExceptionHandler.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\orders\\web\\OrderWebController.java\n",
      "[INFO] Analyzing monolith_code\\src\\main\\java\\com\\sivalabs\\bookstore\\orders\\web\\OrderWebSupport.java\n",
      "[INFO] Analyzing monolith_code\\src\\test\\java\\com\\sivalabs\\bookstore\\TestBookStoreApplication.java\n",
      "[INFO] Analyzing monolith_code\\src\\test\\java\\com\\sivalabs\\bookstore\\TestcontainersConfiguration.java\n",
      "[SUCCESS] Analysis saved to file_analysis.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import threading\n",
    "import openai\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from queue import Queue\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPEN_AI_API_KEY\")\n",
    "\n",
    "MAX_CHARS = 8000\n",
    "max_threads = 8\n",
    "skip_files = ('package-info.java', 'module-info.java', 'pom.xml', 'Dockerfile', '.gitignore', 'README.md', '.git', \"Test.java\", \"Tests.java\")\n",
    "\n",
    "def read_code_file(file_path: Path) -> List[str]:\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        code = f.read()\n",
    "    return [code[i:i + MAX_CHARS] for i in range(0, len(code), MAX_CHARS)]\n",
    "\n",
    "def analyze_chunk(chunk: str, file_name: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "        You are a software architect assistant. Analyze the following code chunk from a file named '{file_name}'.\n",
    "        Return a JSON object with the following fields:\n",
    "        - \"internal_dependencies\": list of filenames this code depends on\n",
    "        - \"external_dependencies\": list of libraries or packages it uses\n",
    "        - \"functions\": an array of descriptions of the functions in this code\n",
    "\n",
    "        Code:\n",
    "        ```\n",
    "        {chunk}\n",
    "        ```\n",
    "\n",
    "        Only return the JSON, nothing else.\n",
    "        \"\"\"\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def worker(queue: Queue, results: List[dict], lock: threading.Lock):\n",
    "    while not queue.empty():\n",
    "        file_path = queue.get()\n",
    "        file = file_path.name\n",
    "\n",
    "        print(f\"[INFO] Analyzing {file_path}\")\n",
    "        try:\n",
    "            file_chunks = read_code_file(file_path)\n",
    "            combined_analysis = {\n",
    "                \"file_name\": file,\n",
    "                \"file_path\": str(file_path),\n",
    "                \"internal_dependencies\": [],\n",
    "                \"external_dependencies\": [],\n",
    "                \"functions\": []\n",
    "            }\n",
    "\n",
    "            for chunk in file_chunks:\n",
    "                try:\n",
    "                    analysis = analyze_chunk(chunk, file)\n",
    "                    data = json.loads(analysis)\n",
    "                    combined_analysis[\"internal_dependencies\"].extend(data.get(\"internal_dependencies\", []))\n",
    "                    combined_analysis[\"external_dependencies\"].extend(data.get(\"external_dependencies\", []))\n",
    "                    combined_analysis[\"functions\"].extend(data.get(\"functions\", []))\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] Failed analyzing chunk from {file}: {e}\")\n",
    "\n",
    "            # Remove duplicates\n",
    "            combined_analysis[\"internal_dependencies\"] = list(set(combined_analysis[\"internal_dependencies\"]))\n",
    "            combined_analysis[\"external_dependencies\"] = list(set(combined_analysis[\"external_dependencies\"]))\n",
    "\n",
    "            with lock:\n",
    "                results.append(combined_analysis)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed analyzing file {file_path}: {e}\")\n",
    "        finally:\n",
    "            queue.task_done()\n",
    "\n",
    "def analyze_repo_code(repo_path: str, output_json_path: str):\n",
    "    file_queue = Queue()\n",
    "    results = []\n",
    "    lock = threading.Lock()\n",
    "\n",
    "    # Populate queue with all valid code files\n",
    "    for root, _, files in os.walk(repo_path):\n",
    "        for file in files:\n",
    "            if file.endswith(('.py', '.js', '.ts', '.java', '.go', '.rb')) and not file.endswith(skip_files):\n",
    "                file_queue.put(Path(root) / file)\n",
    "\n",
    "    # Start worker threads\n",
    "    threads = []\n",
    "    for _ in range(min(max_threads, file_queue.qsize())):\n",
    "        t = threading.Thread(target=worker, args=(file_queue, results, lock))\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    # Wait for all threads to finish\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "\n",
    "    # Save output\n",
    "    with open(output_json_path, 'w') as out_f:\n",
    "        json.dump(results, out_f, indent=2)\n",
    "    print(f\"[SUCCESS] Analysis saved to {output_json_path}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_repo_code(\"./monolith_code\", \"file_analysis.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_code(response_text: str):\n",
    "    response_text = response_text.strip()\n",
    "    \n",
    "    parts = response_text.split(\"```\")\n",
    "    if len(parts) > 1:\n",
    "        response_text = parts[1].strip()\n",
    "    \n",
    "    if response_text.lower().startswith(\"python\"):\n",
    "        response_text = response_text.split(\"\\n\", 1)[-1].strip()\n",
    "        \n",
    "    if response_text.lower().startswith(\"JSON\"):\n",
    "        response_text = response_text.split(\"\\n\", 1)[-1].strip()\n",
    "    \n",
    "    if \"Note that\" in response_text:\n",
    "        response_text = response_text.split(\"Note that\", 1)[0].strip()\n",
    "    \n",
    "    response_text = response_text.replace(\"```\", \"\").strip()\n",
    "    \n",
    "    return response_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h3>Micro-service Identification</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install \"autogen-agentchat[teachable]~=0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\n",
      "LOADING MEMORY FROM DISK\u001b[0m\n",
      "\u001b[92m    Location = ./tmp/interactive/teachability_db\\uid_text_dict.pkl\u001b[0m\n",
      "\u001b[33mteachable_agent\u001b[0m (to user):\n",
      "\n",
      "Hi, I'm a teachable user assistant! What's on your mind?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser\u001b[0m (to teachable_agent):\n",
      "\n",
      "i am shubham\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mteachable_agent\u001b[0m (to user):\n",
      "\n",
      "Nice to meet you, Shubham! How can I assist you today?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (4ce2a270-63d1-4915-8f92-094e59b74955): User requested to end the conversation\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': \"Hi, I'm a teachable user assistant! What's on your mind?\", 'role': 'assistant', 'name': 'teachable_agent'}, {'content': 'i am shubham', 'role': 'user', 'name': 'user'}, {'content': 'Nice to meet you, Shubham! How can I assist you today?', 'role': 'assistant', 'name': 'teachable_agent'}], summary='Nice to meet you, Shubham! How can I assist you today?', cost={'usage_including_cached_inference': {'total_cost': 0.0029100000000000003, 'gpt-4-0613': {'cost': 0.0029100000000000003, 'prompt_tokens': 63, 'completion_tokens': 17, 'total_tokens': 80}}, 'usage_excluding_cached_inference': {'total_cost': 0.0029100000000000003, 'gpt-4-0613': {'cost': 0.0029100000000000003, 'prompt_tokens': 63, 'completion_tokens': 17, 'total_tokens': 80}}}, human_input=[])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from autogen import UserProxyAgent, config_list_from_json\n",
    "from autogen.agentchat.contrib.capabilities.teachability import Teachability\n",
    "from autogen import ConversableAgent \n",
    "\n",
    "filter_dict = {\"model\": [\"gpt-4\"]}\n",
    "llm_config = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"gpt-4\",\n",
    "            \"api_key\": os.getenv(\"OPEN_AI_API_KEY\")\n",
    "        }\n",
    "    ],\n",
    "    \"timeout\": 120\n",
    "}\n",
    "\n",
    "teachable_agent = ConversableAgent(\n",
    "    name=\"teachable_agent\", \n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "teachability = Teachability(\n",
    "    reset_db=False, \n",
    "    path_to_db_dir=\"./tmp/interactive/teachability_db\" \n",
    ")\n",
    "\n",
    "teachability.add_to_agent(teachable_agent)\n",
    "\n",
    "user = UserProxyAgent(\"user\", human_input_mode=\"ALWAYS\")\n",
    "\n",
    "teachable_agent.initiate_chat(user, message=\"Hi, I'm a teachable user assistant! What's on your mind?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from autogen import  UserProxyAgent, AssistantAgent, GroupChatManager\n",
    "from autogen.agentchat.contrib.capabilities.teachability import Teachability\n",
    "\n",
    "filter_dict = {\"model\": [\"gpt-4\"]}\n",
    "llm_config = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"gpt-4\",\n",
    "            \"api_key\": os.getenv(\"OPEN_AI_API_KEY\")\n",
    "        }\n",
    "    ],\n",
    "    \"timeout\": 120\n",
    "}\n",
    "\n",
    "# teachable_agent = ConversableAgent(\n",
    "#     name=\"teachable_agent\", \n",
    "#     llm_config=llm_config,\n",
    "#     human_input_mode=\"NEVER\"\n",
    "# )\n",
    "def process_json_chunks(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    for chunk in data:\n",
    "        chunk_text = json.dumps(chunk)\n",
    "        teachable_agent.initiate_chat(\n",
    "            recipient=teachable_agent,\n",
    "            message=f\"Please learn and summarize this information: {chunk_text}\",\n",
    "            silent=True\n",
    "        )\n",
    "    \n",
    "    final_summary = teachable_agent.initiate_chat(\n",
    "        recipient=teachable_agent,\n",
    "        message=\"Please provide a comprehensive summary of all the information you've learned.\",\n",
    "        silent=True\n",
    "    )\n",
    "    \n",
    "    return final_summary\n",
    "\n",
    "summary_agent = AssistantAgent(\n",
    "    name=\"teachable_agent\", \n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    system_message=\"\"\"You are an assistant agent whose only purpose is to summarize data\"\"\",\n",
    "    function_map={\"process_json_chunks\": process_json_chunks}\n",
    ")\n",
    "\n",
    "# teachability = Teachability(\n",
    "#     reset_db=False, \n",
    "#     path_to_db_dir=\"./tmp/interactive/teachability_db\" \n",
    "# )\n",
    "# teachability.add_to_agent(teachable_agent)\n",
    "\n",
    "# def process_json_chunks(file_path):\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         data = json.load(f)\n",
    "    \n",
    "#     for chunk in data:\n",
    "#         chunk_text = json.dumps(chunk)\n",
    "#         teachable_agent.initiate_chat(\n",
    "#             recipient=teachable_agent,\n",
    "#             message=f\"Please learn and summarize this information: {chunk_text}\",\n",
    "#             silent=True\n",
    "#         )\n",
    "    \n",
    "#     final_summary = teachable_agent.initiate_chat(\n",
    "#         recipient=teachable_agent,\n",
    "#         message=\"Please provide a comprehensive summary of all the information you've learned.\",\n",
    "#         silent=True\n",
    "#     )\n",
    "    \n",
    "#     return final_summary\n",
    "\n",
    "# summary = process_json_chunks(\"file_analysis.json\")\n",
    "# print(\"\\nFinal Summary:\")\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\n",
      "LOADING MEMORY FROM DISK\u001b[0m\n",
      "\u001b[92m    Location = ./tmp/interactive/teachability_db\\uid_text_dict.pkl\u001b[0m\n",
      "[INFO] Processing 43 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 100%|██████████| 43/43 [00:00<00:00, 14337.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] ❌ ProductRepository.java -> Error: list index out of range\n",
      "[2] ❌ BookStoreApplication.java -> Error: list index out of range\n",
      "[3] ❌ ProductNotFoundException.java -> Error: list index out of range\n",
      "[4] ❌ ProductDto.java -> Error: list index out of range\n",
      "[5] ❌ ProductMapper.java -> Error: list index out of range\n",
      "[6] ❌ ProductApi.java -> Error: list index out of range\n",
      "[7] ❌ ProductService.java -> Error: list index out of range\n",
      "[8] ❌ CatalogExceptionHandler.java -> Error: list index out of range\n",
      "[9] ❌ PagedResult.java -> Error: list index out of range\n",
      "[10] ❌ InventoryRepository.java -> Error: list index out of range\n",
      "[11] ❌ ProductEntity.java -> Error: list index out of range\n",
      "[12] ❌ ProductWebController.java -> Error: list index out of range\n",
      "[13] ❌ ProductRestController.java -> Error: list index out of range\n",
      "[14] ❌ InventoryEntity.java -> Error: list index out of range\n",
      "[15] ❌ OrderEventNotificationHandler.java -> Error: list index out of range\n",
      "[16] ❌ CreateOrderResponse.java -> Error: list index out of range\n",
      "[17] ❌ CreateOrderRequest.java -> Error: list index out of range\n",
      "[18] ❌ OrderEventInventoryHandler.java -> Error: list index out of range\n",
      "[19] ❌ RabbitMQConfig.java -> Error: list index out of range\n",
      "[20] ❌ InvalidOrderException.java -> Error: list index out of range\n",
      "[21] ❌ InventoryService.java -> Error: list index out of range\n",
      "[22] ❌ Customer.java -> Error: list index out of range\n",
      "[23] ❌ OrderNotFoundException.java -> Error: list index out of range\n",
      "[24] ❌ OrderView.java -> Error: list index out of range\n",
      "[25] ❌ OrderDto.java -> Error: list index out of range\n",
      "[26] ❌ OrderStatus.java -> Error: list index out of range\n",
      "[27] ❌ OrderCreatedEvent.java -> Error: list index out of range\n",
      "[28] ❌ OrderRepository.java -> Error: list index out of range\n",
      "[29] ❌ OrderItem.java -> Error: list index out of range\n",
      "[30] ❌ OrderForm.java -> Error: list index out of range\n",
      "[31] ❌ OrderService.java -> Error: list index out of range\n",
      "[32] ❌ CartUtil.java -> Error: list index out of range\n",
      "[33] ❌ OrdersApi.java -> Error: list index out of range\n",
      "[34] ❌ Cart.java -> Error: list index out of range\n",
      "[35] ❌ OrderMapper.java -> Error: list index out of range\n",
      "[36] ❌ OrderWebSupport.java -> Error: list index out of range\n",
      "[37] ❌ TestBookStoreApplication.java -> Error: list index out of range\n",
      "[38] ❌ OrdersExceptionHandler.java -> Error: list index out of range\n",
      "[39] ❌ CartController.java -> Error: list index out of range\n",
      "[40] ❌ OrderEntity.java -> Error: list index out of range\n",
      "[41] ❌ TestcontainersConfiguration.java -> Error: list index out of range\n",
      "[42] ❌ OrderRestController.java -> Error: list index out of range\n",
      "[43] ❌ OrderWebController.java -> Error: list index out of range\n",
      "\n",
      "[SUCCESS] Microservice suggestions saved to microservices_list.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from autogen import ConversableAgent\n",
    "from autogen.agentchat.contrib.capabilities.teachability import Teachability\n",
    "\n",
    "# Set up agent and teachability\n",
    "llm_config = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"gpt-4\",\n",
    "            \"api_key\": os.getenv(\"OPEN_AI_API_KEY\")\n",
    "        }\n",
    "    ],\n",
    "    \"timeout\": 120\n",
    "}\n",
    "\n",
    "teachable_agent = ConversableAgent(\n",
    "    name=\"teachable_agent\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\"\n",
    ")\n",
    "\n",
    "teachability = Teachability(\n",
    "    reset_db=False,\n",
    "    path_to_db_dir=\"./tmp/interactive/teachability_db\"\n",
    ")\n",
    "teachability.add_to_agent(teachable_agent)\n",
    "\n",
    "# Load file analysis data\n",
    "with open(\"file_analysis.json\", \"r\") as f:\n",
    "    analysis_data = json.load(f)\n",
    "\n",
    "# Microservice classification\n",
    "microservices = []\n",
    "unique_services = set()\n",
    "\n",
    "print(\"[INFO] Processing\", len(analysis_data), \"files...\")\n",
    "\n",
    "for idx, file in enumerate(tqdm(analysis_data, desc=\"Processing Files\")):\n",
    "    file_name = file.get(\"file_name\", \"UnknownFile\")\n",
    "    functions = str(file.get(\"functions\", \"\"))[:1000]\n",
    "    internal_deps = file.get(\"internal_dependencies\", [])[:10]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are designing a microservices architecture. Below is a summary of a code file:\n",
    "- File: {file_name}\n",
    "- Internal Dependencies: {internal_deps}\n",
    "- Functions: {functions}\n",
    "\n",
    "Based on this, decide how to group this file into a microservice. Consider cohesion, dependencies, and overall design.\n",
    "Only generate services that are necessary — keep total count between 6 to 9 services.\n",
    "\n",
    "Return a JSON in the format:\n",
    "{{\n",
    "  \"suggested_microservice\": \"name\",\n",
    "  \"reason\": \"short explanation why this file belongs there\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        chat_result = teachable_agent.generate_reply(sender=teachable_agent, prompt=prompt)\n",
    "        message = chat_result.message.content.strip()\n",
    "\n",
    "        # Clean markdown wrapping\n",
    "        if message.startswith(\"```json\"):\n",
    "            message = message.split(\"```json\")[-1].split(\"```\")[0].strip()\n",
    "\n",
    "        result = json.loads(message)\n",
    "\n",
    "        if not isinstance(result, dict) or \"suggested_microservice\" not in result:\n",
    "            raise ValueError(\"Invalid format received from agent\")\n",
    "\n",
    "        microservices.append({\n",
    "            \"file_name\": file_name,\n",
    "            \"microservice\": result[\"suggested_microservice\"],\n",
    "            \"reason\": result[\"reason\"]\n",
    "        })\n",
    "\n",
    "        unique_services.add(result[\"suggested_microservice\"])\n",
    "        print(f\"[{idx+1}] ✅ {file_name} → {result['suggested_microservice']} ({time.time() - start_time:.1f}s)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{idx+1}] ❌ {file_name} -> Error: {e}\")\n",
    "\n",
    "# Save result\n",
    "with open(\"microservices_list.json\", \"w\") as f:\n",
    "    json.dump(microservices, f, indent=2)\n",
    "\n",
    "print(f\"\\n[SUCCESS] Microservice suggestions saved to microservices_list.json\")\n",
    "print(f\"Unique microservices generated: {len(unique_services)} → {list(unique_services)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-4.0.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence_transformers)\n",
      "  Downloading transformers-4.51.2-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\desktop\\share\\product-2\\venv\\lib\\site-packages (from sentence_transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence_transformers)\n",
      "  Using cached torch-2.6.0-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting scikit-learn (from sentence_transformers)\n",
      "  Using cached scikit_learn-1.6.1-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting scipy (from sentence_transformers)\n",
      "  Using cached scipy-1.15.2-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\asus\\desktop\\share\\product-2\\venv\\lib\\site-packages (from sentence_transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\asus\\desktop\\share\\product-2\\venv\\lib\\site-packages (from sentence_transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\asus\\desktop\\share\\product-2\\venv\\lib\\site-packages (from sentence_transformers) (4.13.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\desktop\\share\\product-2\\venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asus\\desktop\\share\\product-2\\venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\asus\\desktop\\share\\product-2\\venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\desktop\\share\\product-2\\venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\desktop\\share\\product-2\\venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Collecting networkx (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\desktop\\share\\product-2\\venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\desktop\\share\\product-2\\venv\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\desktop\\share\\product-2\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.2.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\desktop\\share\\product-2\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\asus\\desktop\\share\\product-2\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.1)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence_transformers)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence_transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence_transformers)\n",
      "  Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\desktop\\share\\product-2\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\desktop\\share\\product-2\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\desktop\\share\\product-2\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\desktop\\share\\product-2\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.1.31)\n",
      "Downloading sentence_transformers-4.0.2-py3-none-any.whl (340 kB)\n",
      "   ---------------------------------------- 0.0/340.6 kB ? eta -:--:--\n",
      "   ------------------------------------- -- 317.4/340.6 kB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 340.6/340.6 kB 7.0 MB/s eta 0:00:00\n",
      "Using cached torch-2.6.0-cp311-cp311-win_amd64.whl (204.2 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Downloading transformers-4.51.2-py3-none-any.whl (10.4 MB)\n",
      "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/10.4 MB 14.5 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 1.0/10.4 MB 12.9 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 1.2/10.4 MB 12.7 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 2.0/10.4 MB 11.7 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.6/10.4 MB 11.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.0/10.4 MB 11.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.5/10.4 MB 11.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.0/10.4 MB 11.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.4/10.4 MB 10.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.7/10.4 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.1/10.4 MB 10.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.3/10.4 MB 9.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.4/10.4 MB 9.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.6/10.4 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.8/10.4 MB 8.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.2/10.4 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.5/10.4 MB 8.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.9/10.4 MB 8.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.2/10.4 MB 8.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.4/10.4 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.4/10.4 MB 8.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.6/10.4 MB 7.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.0/10.4 MB 7.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.2/10.4 MB 7.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.6/10.4 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.0/10.4 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.4/10.4 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.6/10.4 MB 7.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.9/10.4 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.3/10.4 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.4/10.4 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.4/10.4 MB 7.2 MB/s eta 0:00:00\n",
      "Using cached scikit_learn-1.6.1-cp311-cp311-win_amd64.whl (11.1 MB)\n",
      "Using cached scipy-1.15.2-cp311-cp311-win_amd64.whl (41.2 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, sympy, scipy, safetensors, networkx, MarkupSafe, joblib, scikit-learn, jinja2, torch, transformers, sentence_transformers\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "Successfully installed MarkupSafe-3.0.2 jinja2-3.1.6 joblib-1.4.2 networkx-3.4.2 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.15.2 sentence_transformers-4.0.2 sympy-1.13.1 threadpoolctl-3.6.0 torch-2.6.0 transformers-4.51.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Desktop\\share\\product-2\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\ASUS\\Desktop\\share\\product-2\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASUS\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 JSON split into 23 chunks. Generating summaries...\n",
      "\n",
      "🧠 Summarizing chunk 1/23...\n",
      "🧠 Summarizing chunk 2/23...\n",
      "🧠 Summarizing chunk 3/23...\n",
      "🧠 Summarizing chunk 4/23...\n",
      "🧠 Summarizing chunk 5/23...\n",
      "🧠 Summarizing chunk 6/23...\n",
      "🧠 Summarizing chunk 7/23...\n",
      "🧠 Summarizing chunk 8/23...\n",
      "🧠 Summarizing chunk 9/23...\n",
      "🧠 Summarizing chunk 10/23...\n",
      "🧠 Summarizing chunk 11/23...\n",
      "🧠 Summarizing chunk 12/23...\n",
      "🧠 Summarizing chunk 13/23...\n",
      "🧠 Summarizing chunk 14/23...\n",
      "🧠 Summarizing chunk 15/23...\n",
      "🧠 Summarizing chunk 16/23...\n",
      "🧠 Summarizing chunk 17/23...\n",
      "🧠 Summarizing chunk 18/23...\n",
      "🧠 Summarizing chunk 19/23...\n",
      "🧠 Summarizing chunk 20/23...\n",
      "🧠 Summarizing chunk 21/23...\n",
      "🧠 Summarizing chunk 22/23...\n",
      "🧠 Summarizing chunk 23/23...\n",
      "\n",
      "📦 Generating final summary from chunk summaries...\n",
      "\n",
      "\n",
      "📝 Final Summary:\n",
      "The text discusses various Java files and their functionalities in a bookstore application. These include files for product and order management, exception handling, and dependencies. Some of the notable files include \"ProductRepository.java\", \"BookStoreApplication.java\", \"ProductNotFoundException.java\", and \"ProductDto.java\". The files contain methods for finding products by code, starting the application, handling exceptions, and getting product display names. Other files handle tasks like mapping product entities to product DTOs, initializing services, retrieving products, and handling exceptions. The application also includes files for managing inventory, setting product details, and handling order events. The text also mentions a RabbitMQ configuration, inventory management, and order processing. The last part of the text discusses adding OrderDto to a model and returning the name of the view to be rendered.\n",
      "\n",
      "✅ Stored in ChromaDB with ID: 1915520094111673\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import chromadb\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb.utils.embedding_functions import DefaultEmbeddingFunction\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "# === Load environment variables ===\n",
    "load_dotenv()\n",
    "OPEN_API_KEY = os.getenv(\"OPEN_AI_API_KEY\")\n",
    "\n",
    "# === Config ===\n",
    "OPEN_API_URL = \"https://api.openai.com/v1/chat/completions\"\n",
    "GROQ_MODEL = \"gpt-4\"\n",
    "EMBED_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 2000  # characters per chunk\n",
    "\n",
    "# === SentenceTransformer Embedding Function ===\n",
    "class LocalEmbeddingFunction(DefaultEmbeddingFunction):\n",
    "    def __init__(self, model_name=EMBED_MODEL_NAME):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def __call__(self, texts):\n",
    "        return self.model.encode(texts).tolist()\n",
    "\n",
    "embedding_func = LocalEmbeddingFunction()\n",
    "\n",
    "# === Setup ChromaDB ===\n",
    "client = chromadb.Client()\n",
    "collection = client.get_or_create_collection(\"summaries\", embedding_function=embedding_func)\n",
    "\n",
    "# === Use AutoGen Agent to call Groq ===\n",
    "class GroqAgent(ConversableAgent):\n",
    "    def summarize(self, content: str) -> str:\n",
    "        payload = {\n",
    "            \"model\": GROQ_MODEL,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes text.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Summarize the following:\\n\\n{content}\"}\n",
    "            ],\n",
    "            \"temperature\": 0.5,\n",
    "            \"max_tokens\": 300\n",
    "        }\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {OPEN_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        response = requests.post(OPEN_API_URL, headers=headers, json=payload)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        else:\n",
    "            raise Exception(f\"Groq API Error: {response.status_code} - {response.text}\")\n",
    "\n",
    "agent = GroqAgent(name=\"groq-agent\")\n",
    "\n",
    "# === Chunking Function ===\n",
    "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# === Store in Chroma ===\n",
    "def store_summary(original: str, summary: str, metadata: dict = {}):\n",
    "    doc_id = str(abs(hash(original)))[:16]\n",
    "    collection.add(\n",
    "        documents=[original],\n",
    "        metadatas=[{\"summary\": summary, **metadata}],\n",
    "        ids=[doc_id]\n",
    "    )\n",
    "    print(f\"\\n✅ Stored in ChromaDB with ID: {doc_id}\")\n",
    "\n",
    "# === Query Chroma ===\n",
    "def query_summary(query: str, top_k: int = 3):\n",
    "    results = collection.query(query_texts=[query], n_results=top_k)\n",
    "    print(\"\\n📚 Top Results:\\n\")\n",
    "    for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "        print(f\"Document:\\n{doc}\\nSummary: {meta['summary']}\\n---\\n\")\n",
    "\n",
    "# === Main Flow ===\n",
    "def main():\n",
    "    mode = input(\"Enter mode (summarize / retrieve): \").strip().lower()\n",
    "\n",
    "    if mode == \"summarize\":\n",
    "        path = input(\"Enter path to .json file: \").strip()\n",
    "        if not os.path.isfile(path) or not path.endswith(\".json\"):\n",
    "            print(\"❌ Invalid file path.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                full_text = json.dumps(data, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error reading JSON: {e}\")\n",
    "            return\n",
    "\n",
    "        chunks = chunk_text(full_text)\n",
    "        print(f\"\\n🔍 JSON split into {len(chunks)} chunks. Generating summaries...\\n\")\n",
    "        chunk_summaries = []\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"🧠 Summarizing chunk {i+1}/{len(chunks)}...\")\n",
    "            summary = agent.summarize(chunk)\n",
    "            chunk_summaries.append(summary)\n",
    "\n",
    "        final_input = \"\\n\\n\".join([f\"Chunk {i+1} Summary: {s}\" for i, s in enumerate(chunk_summaries)])\n",
    "        print(\"\\n📦 Generating final summary from chunk summaries...\\n\")\n",
    "        final_summary = agent.summarize(final_input)\n",
    "\n",
    "        print(f\"\\n📝 Final Summary:\\n{final_summary}\")\n",
    "        store_summary(full_text, final_summary, metadata={\"source_file\": path})\n",
    "\n",
    "    elif mode == \"retrieve\":\n",
    "        query = input(\"Enter your search query: \")\n",
    "        query_summary(query)\n",
    "\n",
    "    else:\n",
    "        print(\"❌ Invalid mode. Choose 'summarize' or 'retrieve'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "An instance of Chroma already exists for ephemeral with different settings",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m embedding_func = LocalEmbeddingFunction()\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# === Setup ChromaDB ===\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m client = \u001b[43mchromadb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchromadb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSettings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./chroma_storage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manonymized_telemetry\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m collection = client.get_or_create_collection(\u001b[33m\"\u001b[39m\u001b[33mmicroservices\u001b[39m\u001b[33m\"\u001b[39m, embedding_function=embedding_func)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# === Use AutoGen Agent to call Groq ===\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\share\\product-2\\venv\\Lib\\site-packages\\chromadb\\__init__.py:371\u001b[39m, in \u001b[36mClient\u001b[39m\u001b[34m(settings, tenant, database)\u001b[39m\n\u001b[32m    368\u001b[39m tenant = \u001b[38;5;28mstr\u001b[39m(tenant)\n\u001b[32m    369\u001b[39m database = \u001b[38;5;28mstr\u001b[39m(database)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mClientCreator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[43m=\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\share\\product-2\\venv\\Lib\\site-packages\\chromadb\\api\\client.py:61\u001b[39m, in \u001b[36mClient.__init__\u001b[39m\u001b[34m(self, tenant, database, settings)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     56\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     57\u001b[39m     tenant: \u001b[38;5;28mstr\u001b[39m = DEFAULT_TENANT,\n\u001b[32m     58\u001b[39m     database: \u001b[38;5;28mstr\u001b[39m = DEFAULT_DATABASE,\n\u001b[32m     59\u001b[39m     settings: Settings = Settings(),\n\u001b[32m     60\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m=\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28mself\u001b[39m.tenant = tenant\n\u001b[32m     63\u001b[39m     \u001b[38;5;28mself\u001b[39m.database = database\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\share\\product-2\\venv\\Lib\\site-packages\\chromadb\\api\\shared_system_client.py:19\u001b[39m, in \u001b[36mSharedSystemClient.__init__\u001b[39m\u001b[34m(self, settings)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     16\u001b[39m     settings: Settings = Settings(),\n\u001b[32m     17\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mself\u001b[39m._identifier = SharedSystemClient._get_identifier_from_settings(settings)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[43mSharedSystemClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create_system_if_not_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_identifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\Desktop\\share\\product-2\\venv\\Lib\\site-packages\\chromadb\\api\\shared_system_client.py:38\u001b[39m, in \u001b[36mSharedSystemClient._create_system_if_not_exists\u001b[39m\u001b[34m(cls, identifier, settings)\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# For now, the settings must match\u001b[39;00m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m previous_system.settings != settings:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     39\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAn instance of Chroma already exists for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with different settings\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     40\u001b[39m         )\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._identifier_to_system[identifier]\n",
      "\u001b[31mValueError\u001b[39m: An instance of Chroma already exists for ephemeral with different settings"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import chromadb\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb.utils.embedding_functions import DefaultEmbeddingFunction\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "# === Load environment variables ===\n",
    "load_dotenv()\n",
    "OPEN_API_KEY = os.getenv(\"OPEN_AI_API_KEY\")\n",
    "\n",
    "# === Config ===\n",
    "OPEN_API_URL = \"https://api.openai.com/v1/chat/completions\"\n",
    "GROQ_MODEL = \"gpt-4\"\n",
    "EMBED_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 18000  # characters per chunk\n",
    "\n",
    "# === SentenceTransformer Embedding Function ===\n",
    "class LocalEmbeddingFunction(DefaultEmbeddingFunction):\n",
    "    def __init__(self, model_name=EMBED_MODEL_NAME):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def __call__(self, texts):\n",
    "        return self.model.encode(texts).tolist()\n",
    "\n",
    "embedding_func = LocalEmbeddingFunction()\n",
    "\n",
    "# === Setup ChromaDB ===\n",
    "client = chromadb.Client()\n",
    "collection = client.get_or_create_collection(\"microservices\", embedding_function=embedding_func)\n",
    "\n",
    "# === Use AutoGen Agent to call Groq ===\n",
    "class GroqAgent(ConversableAgent):\n",
    "    def generate_microservices(self, content: str) -> str:\n",
    "        payload = {\n",
    "            \"model\": GROQ_MODEL,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You are a software architect helping to migrate a monolith to microservices. \"\n",
    "                        \"Given code and related analysis, suggest which microservice this content belongs to, \"\n",
    "                        \"what its role would be, and what service boundaries it should respect.\"\n",
    "                    )\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": content}\n",
    "            ],\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_tokens\": 500\n",
    "        }\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {OPEN_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        response = requests.post(OPEN_API_URL, headers=headers, json=payload)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        else:\n",
    "            raise Exception(f\"Groq API Error: {response.status_code} - {response.text}\")\n",
    "\n",
    "agent = GroqAgent(name=\"microservice-agent\")\n",
    "\n",
    "# === Chunking Function ===\n",
    "def chunk_text(text: str, chunk_size: int = CHUNK_SIZE):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# === Store in ChromaDB ===\n",
    "def store_microservice_mapping(original: str, mapping: str, metadata: dict = {}):\n",
    "    doc_id = str(abs(hash(original)))[:16]\n",
    "    collection.add(\n",
    "        documents=[original],\n",
    "        metadatas=[{\"microservice_mapping\": mapping, **metadata}],\n",
    "        ids=[doc_id]\n",
    "    )\n",
    "    print(f\"\\n✅ Stored microservice mapping in ChromaDB with ID: {doc_id}\")\n",
    "\n",
    "# === Query ChromaDB ===\n",
    "def query_microservices(query: str, top_k: int = 3):\n",
    "    results = collection.query(query_texts=[query], n_results=top_k)\n",
    "    print(\"\\n🔍 Top Microservice Mapping Results:\\n\")\n",
    "    for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "        print(f\"Document Snippet:\\n{doc[:300]}...\\nMapping: {meta['microservice_mapping']}\\n---\\n\")\n",
    "\n",
    "# === Main Flow for Step 3 ===\n",
    "def main():\n",
    "    mode = input(\"Enter mode (generate / retrieve): \").strip().lower()\n",
    "\n",
    "    if mode == \"generate\":\n",
    "        path = input(\"Enter path to .json file (file analysis): \").strip()\n",
    "        if not os.path.isfile(path) or not path.endswith(\".json\"):\n",
    "            print(\"❌ Invalid file path.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                full_text = json.dumps(data, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error reading JSON: {e}\")\n",
    "            return\n",
    "\n",
    "        chunks = chunk_text(full_text)\n",
    "        print(f\"\\n📦 File analysis split into {len(chunks)} chunks. Generating microservice mappings...\\n\")\n",
    "        microservice_mappings = []\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"🔧 Processing chunk {i+1}/{len(chunks)}...\")\n",
    "            mapping = agent.generate_microservices(chunk)\n",
    "            microservice_mappings.append(mapping)\n",
    "\n",
    "        all_mappings_text = \"\\n\\n\".join([f\"Chunk {i+1} Mapping: {m}\" for i, m in enumerate(microservice_mappings)])\n",
    "        print(\"\\n📦 Generating final microservice organization...\\n\")\n",
    "        final_mapping = agent.generate_microservices(all_mappings_text)\n",
    "\n",
    "        print(f\"\\n🧭 Final Microservice Architecture:\\n{final_mapping}\")\n",
    "        store_microservice_mapping(full_text, final_mapping, metadata={\"source_file\": path})\n",
    "\n",
    "    elif mode == \"retrieve\":\n",
    "        query = input(\"Enter your search query (e.g., 'order service'): \")\n",
    "        query_microservices(query)\n",
    "\n",
    "    else:\n",
    "        print(\"❌ Invalid mode. Choose 'generate' or 'retrieve'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
